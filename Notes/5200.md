###### Apache Hadoop

- Software Framework for storing, processing, analyzing big data
	- Distributed
	- Scalable
	- Fault-tolerant
	- Open source

- Open Source vs Commercial Hadoop Vendors

Vanilla 		 | Distribution, including Apache Hadoop
--------------|----------------------------------------
Apache Hadoop | Cloudera CDH Hadoop Distribution
 				 | Hortonworks Data Platform (HDP)
				 | MapR Hadoop Distribution
				 | Amazon Elastic MapReduce
 				 | IBM Open Platform
				 | Microsoft Azure's HDInsight -Cloud based Hadoop Distrbution
				 | Pivotal Big Data Suite
				 | Datameer Professional
				 | Datastax Enterprise Analytics
				 | Dell-Cloudera Apache Hadoop Solution

- Advantages of Commercial Hadoop Vendors

	- Support
	- Reliability
	- Completeness

- Motivation for Hadoop

	- Problem &rarr; Traditional Large-Scale Computation
		- processor-bound
		- bigger computers - Faster processor, more memory is not enough


	- Solution &rarr; Distributed Systems
		- Use multiple machines for a single job
		- ``MPI`` ```Message Passing Interface``` 

- Problem/Challenges with Distributed Systems

	- Data exchange requires _**synchronization**_
	- _**Finite bandwidth**_ is available
	- Temporal _**dependencies**_ are complicated
	- It is difficult to deal with _**partial failures**_ of the system
	- _**Data Bottleneck**_
		- Getting the data to the processors becomes the bottleneck
			- terabytes+ per day
			- petabytes+ total

- Requirements for a New Approach

	- Partial Failure Support
		- Failure of a component should result in a graceful degradation ofapplication performance
		- Not complete failure of the enFre system
	- Data Recoverability
		-  If a component of the system fails, its workload should be assumed by still-functioning units in the system		- Failure should not result in the loss of any data
	- Component Recovery
		- If a component of the system fails and then recovers, it should be able to rejoin the system		- Without requiring a full restart of the entire system
	- Consistency
		- Component failures during execution of a job should not affect the outcome of the job
	- Scalability
		- Increasing resources should support a proportional increase in loadcapacity
		- Adding load to the system should result in a graceful decline in performance of individual jobs
			- Not failure of the system

- Solution to Distrubuted Systems
	- Individual nodes can work on data local to those nodes
	- No data transfer over the network is required for initial processing

- Core Hadoop Concepts
	- Applications are written in high-level code
	- Nodes talk to each other as little as possible
	- Data is spread among machines in advance